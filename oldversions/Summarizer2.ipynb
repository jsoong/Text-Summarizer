{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use extractive text summarization using TextRank algorithm which computes a similarity matrix for each sentence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WASHINGTON — Defense Secretary Jim Mattis, whose experience and stability were widely seen as a balance to an unpredictable president, resigned on Thursday in protest of President Trump’s decision to withdraw 2,000 American troops from Syria.']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = open(\"nytimes article.txt\", \"r\")\n",
    "\n",
    "sentences = []\n",
    "for line in article:\n",
    "    line = line.split(\". \")\n",
    "    for i in range(len(line)):\n",
    "        line[i] = line[i].rstrip()\n",
    "    sentences.append(line)\n",
    "    \n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [y for x in sentences for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for s in sentences if len(s) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "new_sentences = []\n",
    "for s in sentences:\n",
    "    new_sentences.append(sent_tokenize(s))\n",
    "\n",
    "new_sentences = [y for x in new_sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_sentences = pd.Series(new_sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentence_vectors)):\n",
    "    sentence_vectors[i] = sentence_vectors[i].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i], sentence_vectors[j])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON — Defense Secretary Jim Mattis, whose experience and stability were widely seen as a balance to an unpredictable president, resigned on Thursday in protest of President Trump’s decision to withdraw 2,000 American troops from Syria.\n",
      "\n",
      "The president said he would name Mr\n",
      "\n",
      "Mattis had told close friends that he would continue in the job despite his deteriorating relationship with Mr\n",
      "\n",
      "Over the past six months, the president and the defense chief have also found themselves at odds over NATO policy, whether to resume large-scale military exercises with South Korea and, privately, whether Mr\n",
      "\n",
      "The president’s tweets announcing the departure of his defense secretary shocked officials at the Pentagon, who as recently as Thursday afternoon were insisting that Mr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(ranked_sentences[i][1], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start 2nd article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt') # one time execution\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A WHOLE LIFETIME in New York City, and Christiana Ting didn’t realize just how many urgent care facilities there were until the app told her to start looking for them',\n",
       " '“They were giving extra points for medical offices, and I found them, I think, on every block,” she says',\n",
       " '“I’m not sure what that says about the neighborhood where I work.”']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = open(\"Google maps article.txt\", \"r\")\n",
    "\n",
    "sentences = []\n",
    "for line in article:\n",
    "    line = line.split(\". \")\n",
    "    for i in range(len(line)):\n",
    "        line[i] = line[i].rstrip()\n",
    "    sentences.append(line)\n",
    "    \n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [y for x in sentences for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for s in sentences if len(s) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "new_sentences = []\n",
    "for s in sentences:\n",
    "    new_sentences.append(sent_tokenize(s))\n",
    "\n",
    "new_sentences = [y for x in new_sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_sentences = pd.Series(new_sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "True"
      ]
     },
     "execution_count": 190,
=======
       "['THIS STARTUP IS CHALLENGING GOOGLE MAPS—AND IT NEEDS YOU',\n",
       " '',\n",
       " \"StreetCred's MapNYC program is an effort to find out what might motivate map enthusiasts, crypto-lovers, maybe even people who hadn’t the faintest about either, to feed it data.MUIRIS WOULFE/GETTY IMAGES\",\n",
       " 'A WHOLE LIFETIME in New York City, and Christiana Ting didn’t realize just how many urgent care facilities there were until the app told her to start looking for them. “They were giving extra points for medical offices, and I found them, I think, on every block,” she says. “I’m not sure what that says about the neighborhood where I work.”',\n",
       " '']"
      ]
     },
     "execution_count": 2,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
=======
    "article = open(\"nytimes article.txt\", \"r\")\n",
    "\n",
    "sentences = []\n",
    "for line in article:\n",
    "    sentences.append(line.rstrip())\n",
    "sentences[:5]"
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 194,
=======
   "execution_count": 3,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "metadata": {
    "collapsed": true
   },
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "source": [
    "for i in range(len(sentence_vectors)):\n",
    "    sentence_vectors[i] = sentence_vectors[i].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 4,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 205,
=======
   "execution_count": 5,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i], sentence_vectors[j])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "The number one spot went to a hedge fund product manager named Max Koenig, who mapped over 200 points of interest per day\n",
      "\n",
      "“Gamification in the OpenStreetMap community has always been highly controversial, because if you’re doing it for money, then when the money goes away, you’re not going to do it anymore, and maybe you’re not doing it for the right reasons,” says Sieber.\n",
      "\n",
      "Either way, it’s static, and that means it’s only a matter of time before it fails to represent reality.\n",
      "\n",
      "StreetCred’s main research question: How do you convince regular people to build and verify mapping data?\n",
      "\n",
      "“There’s a lot of companies, none of whom I can name, who have location data, and that data needs improvement,” says Randy Meech, CEO of the small startup\n",
      "\n"
=======
      "18\n"
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(ranked_sentences[i][1], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start 3rd article"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt') # one time execution\n",
    "import re"
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "new_sentences = []\n",
    "for s in sentences:\n",
    "  new_sentences.append(sent_tokenize(s))\n",
    "\n",
    "new_sentences = [y for x in new_sentences for y in x] # flatten list"
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 7,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "['Abstract']"
      ]
     },
     "execution_count": 262,
=======
       "['THIS STARTUP IS CHALLENGING GOOGLE MAPS—AND IT NEEDS YOU',\n",
       " \"StreetCred's MapNYC program is an effort to find out what might motivate map enthusiasts, crypto-lovers, maybe even people who hadn’t the faintest about either, to feed it data.MUIRIS WOULFE/GETTY IMAGES\",\n",
       " 'A WHOLE LIFETIME in New York City, and Christiana Ting didn’t realize just how many urgent care facilities there were until the app told her to start looking for them.',\n",
       " '“They were giving extra points for medical offices, and I found them, I think, on every block,” she says.',\n",
       " '“I’m not sure what that says about the neighborhood where I work.”']"
      ]
     },
     "execution_count": 7,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = open(\"research paper.txt\", \"r\")\n",
    "\n",
    "sentences = []\n",
    "for line in article:\n",
    "    line = line.split(\". \")\n",
    "    for i in range(len(line)):\n",
    "        line[i] = line[i].rstrip()\n",
    "    sentences.append(line)\n",
    "    \n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [y for x in sentences for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for s in sentences if len(s) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "new_sentences = []\n",
    "for s in sentences:\n",
    "    new_sentences.append(sent_tokenize(s))\n",
    "\n",
    "new_sentences = [y for x in new_sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 8,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 9,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(new_sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 268,
=======
   "execution_count": 10,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jhsoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
<<<<<<< HEAD
     "execution_count": 268,
=======
     "execution_count": 10,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 11,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 12,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 13,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 14,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 273,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 15,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
<<<<<<< HEAD
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_vectors))"
=======
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 16,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "for i in range(len(sentence_vectors)):\n",
    "    sentence_vectors[i] = sentence_vectors[i].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 277,
=======
   "execution_count": 17,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "339\n"
=======
      "56\n"
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
     ]
    }
   ],
   "source": [
    "print(len(sentence_vectors))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 18,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 279,
=======
   "execution_count": 19,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "(339, 339)\n"
=======
      "(18, 18)\n"
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
     ]
    }
   ],
   "source": [
    "print(sim_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 20,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i], sentence_vectors[j])[0,0]"
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i], sentence_vectors[j])[0,0]"
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 22,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 23,
   "metadata": {},
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 284,
=======
   "execution_count": 24,
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "as input, the generator was teacher-forced to predict the human-written summary of source text.\n",
      "\n",
      "However, it is very possible that the generator’s output word sequence can only be processed and recognized by the reconstructor but is\n",
      "\n",
      "in order to reconstruct the source text, it must reflect the core idea of the source text\n",
      "\n",
      "pre-trained language model to constrain the generated sequence to natural language\n",
      "\n",
      "generator was good at selecting the most important words from the articles, but sometimes failed\n",
=======
      "But open-source cartography isn’t always comprehensive or particular enough for the users Meech is targeting. If a company making a VR game for kids needs to know the location of every playground in Cincinnati, there’s no guarantee the volunteers will plug that in. So StreetCred might offer a future mapping army an extra crypto incentive to find, validate, and label those locations.\n",
      "\n",
      "StreetCred sees that as an opportunity. “There’s a lot of companies, none of whom I can name, who have location data, and that data needs improvement,” says Randy Meech, CEO of the small startup. (Meech’s last open-source mapping company, a Samsung subsidiary called Mapzen, shut down in January.) Maybe a client found a data set online or purchased one from another company. Either way, it’s static, and that means it’s only a matter of time before it fails to represent reality.\n",
      "\n",
      "And more accurate, too, with serious assists from cryptocurrency-seeking mappers. Other companies rely on OpenStreetMap, a crowdsourced, open-source effort to create a complete and editable map of the world. (Think Wikipedia for maps.) OSM’s 1 million contributors are constantly adding to and updating its maps using GPS devices, aerial imagery, and info they enter manually.\n",
      "\n",
      "Ting was one of 761 New Yorkers who downloaded, played with, and occasionally became obsessed with an app called MapNYC this fall, vying for their share of an 8-bitcoin prize (worth about $50,000 at the time). The month-long contest, run by a new mapping startup called StreetCred, was really an experiment. StreetCred’s main research question: How do you convince regular people to build and verify mapping data?\n",
      "\n",
      "A WHOLE LIFETIME in New York City, and Christiana Ting didn’t realize just how many urgent care facilities there were until the app told her to start looking for them. “They were giving extra points for medical offices, and I found them, I think, on every block,” she says. “I’m not sure what that says about the neighborhood where I work.”\n",
      "\n",
      "Validated and unvalidated data points in Queens, New York, a few weeks before the conclusion of the MapNYC contest. StreetCred CEO Randy Meech says he suspects Uber and Lyft drivers were adding this data as they picked up, dropped off, and waited for fares near John F. Kennedy International Airport. STREETVIEW\n",
      "\n",
      "The purists and longtime mappers may not love profit-driven cartography, but StreetCred isn’t the only outfit that monetizes open-source map info. Companies like Mapbox, Mapillary, Esri, Telenav, and DigitalGlobe also refine and create visual tools for clients willing to provide or pay for mapping data.\n",
      "\n",
      "Of the StreetCred mappers, Ting was ranked the 25th most prolific. The number one spot went to a hedge fund product manager named Max Koenig, who mapped over 200 points of interest per day. “My weekend was completely monopolized by this,” Koenig says. He’d get up at 8 in the morning and leave his Manhattan home to spend all day charting the outer boroughs, where MapNYC gave bigger bonuses.\n",
      "\n",
      "StreetCred's MapNYC program is an effort to find out what might motivate map enthusiasts, crypto-lovers, maybe even people who hadn’t the faintest about either, to feed it data.MUIRIS WOULFE/GETTY IMAGES\n",
      "\n",
      "For regular folk, detailed, reliable mapping info is helpful. For businesses, it can be crucial. Some want to be found when a map user searches for the nearest sandwich shop. Others use products that rely on base maps—think Uber, the Weather Channel, your car’s navigation system—and require up-to-date location data. “One of the huge challenges to any geographic database is its currency,” says Renee Sieber, a geographer who studies participatory mapping at McGill University. That is to say, yesterday’s map is no good to anybody doing business today.\n",
      "\n",
      "It turns out that the maps that guide you to the nearest Arby’s, or help your Lyft driver find your house, don’t just materialize. “I took mapping for granted until I started the competition,” Ting says, even though she pulls up Google Maps at least twice a day. “But it’s such an inconvenience if the info on the map is wrong, especially in a place like New York, that’s changing all the time.”\n",
      "\n",
      "Google Maps, the giant in this space, has created its extensive database through years of web scraping, Streetview roaming, purchasing and collecting satellite data, and both paying and asking volunteers to verify that the businesses it identifies are still in the same place. But the company doesn’t provide all of its specific location or “point of interest” data to developers—where that Thai restaurant is, or where the hiking trail starts, or where the hospital parking lot is located. When it and other mapping services like HERE Technologies, TomTom, and Foursquare do offer that intel, it can be pricey. StreetCred wants to make that info free for customers who don’t need that much data and cheaper for those that do.\n",
      "\n",
      "StreetCred will need people like Koenig and Ting going forward. And it will have to prove that incentives can keep the map of the world fresh, detailed—and worth paying for.\n",
      "\n",
      "StreetCred’s contest approach is not uncontentious. “Gamification in the OpenStreetMap community has always been highly controversial, because if you’re doing it for money, then when the money goes away, you’re not going to do it anymore, and maybe you’re not doing it for the right reasons,” says Sieber.\n",
      "\n",
      "THIS STARTUP IS CHALLENGING GOOGLE MAPS—AND IT NEEDS YOU\n",
      "\n",
      "Before StreetCred can worry about the philosophical questions, though, it has to prove that its idea works. It plans to run more MapNYC-like experiments in other places, including the Consumer Electronics Show in Las Vegas next month. It plans to create its own cryptocurrency. And it’s still reviewing lessons learned in New York: The company says the MapNYC project generated over 20,000 places in four weeks, some validated by three users.\n",
      "\n",
      "It’s a fun idea, but not easy to pull off. “What StreetCred is trying to solve is really a very hard problem,” says Martijn van Exel, who has been an OpenStreetMap contributor since 2007 and served for years on its board of directors. “Point of interest data requires a very focused effort. It requires a lot of eyes on the ground and people actually observing changes in the field.”\n",
      "\n",
      "Which is why Meech says he came up with MapNYC: an effort to find out what might motivate map enthusiasts, crypto-lovers, maybe even people who hadn’t the faintest about either, to feed it data. The company says it will contribute that info back to OpenStreetMaps under a licensing agreement designed to make it easier for people and companies to share and collaborate when working with data.\n",
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
      "\n"
     ]
    }
   ],
   "source": [
    "#takes 33% of the most relevant content from the original article\n",
    "for i in range(int(len(sentence_vectors)*(1/3))):\n",
    "    print(ranked_sentences[i][1], end=\"\\n\\n\")"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 392a3dfdd0334ea5fd1366bd397ad5f80ae5752a
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
