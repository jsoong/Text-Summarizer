{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform extractive text summarization using the TextRank algorithm which uses a graph-based ranking model which requires no training, using GloVe v1.2 pre-trained word vectors, and performing various natural language preprocessing & tokenization using NLTK library.\n",
    "\n",
    "Graph based ranking algorithms allow knowledge about the text as a whole and the relationship between different parts of a text to be used in making specific local ranking decisions.  It does so by taking into account global information recursively computed from the entire graph to evaluate the importance of a vertex within a graph, rather than relying only on local information.\n",
    "\n",
    "Traditional word vector techniques depend on the distance or angle between pairs of word vectors to determine the strength of a set of word representations.  Glove attempts to uncover more of the language structure by examining not only the scalar difference but various dimensions of difference.  It does this by examining the ratio of the co-occurrence probability between pairings rather than just the probabilities alone.  A weighted least squares regression is then applied to remove the noise.  Dimensionality reduction is applied to the co-occurrence matrix to yield a lower dimensional matrix such that each vector represents a word.\n",
    "\n",
    "Commonly, the ROUGE Metric is used to evaluate summarizer performance, but we did not have immediate resources to test it.  Subjectively, the performance seems reasonably good.  When summarizing NY Times articles, they were significantly shorter (33% of original article length was chosen), were easy to read, and conveyed the most important ideas within the text.\n",
    "\n",
    "This summarizer does not understand or recognize the grammatical structure of language so it cannot extract this type of semantic information and its importance in relating sentences; eg: pronoun references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WASHINGTON — Defense Secretary Jim Mattis, whose experience and stability were widely seen as a balance to an unpredictable president, resigned on Thursday in protest of President Trump’s decision to withdraw 2,000 American troops from Syria.',\n",
       " 'Mr. Trump announced Mr. Mattis’s resignation in two tweets Thursday evening, and said the retired four-star Marine general turned Pentagon chief will leave at the end of February.',\n",
       " 'Officials said Mr. Mattis went to the White House on Thursday afternoon in a last attempt to convince Mr. Trump to keep American troops in Syria, where they have been fighting the Islamic State.',\n",
       " 'He was rebuffed, and told the president that he was resigning as a result.',\n",
       " 'Hours later, the Pentagon released Mr. Mattis’s resignation letter, in which he implicitly criticized his commander in chief.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../Text-Summarizer/articles/nytimes article.txt\", \"r\") as myfile:\n",
    "    article=myfile.read()\n",
    "\n",
    "sentences = sent_tokenize(article)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(sentences):\n",
    "    new_sentences = []\n",
    "    for s in sentences:\n",
    "        new_sentences.append(sent_tokenize(s))\n",
    "\n",
    "    new_sentences = [y for x in new_sentences for y in x] # flatten list\n",
    "    return new_sentences\n",
    "\n",
    "new_sentences = flatten_list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(new_sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jhsoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence_vectors)):\n",
    "    sentence_vectors[i] = sentence_vectors[i].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 24)\n"
     ]
    }
   ],
   "source": [
    "print(sim_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity computes the similarity between vectors based on the degree of orthogonality between vectors where a cosine of 1 is identical and a cosine of 0 is orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i], sentence_vectors[j])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output below: 33% of most relevant content from original article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON — Defense Secretary Jim Mattis, whose experience and stability were widely seen as a balance to an unpredictable president, resigned on Thursday in protest of President Trump’s decision to withdraw 2,000 American troops from Syria.\n",
      "\n",
      "Officials said Mr. Mattis went to the White House on Thursday afternoon in a last attempt to convince Mr. Trump to keep American troops in Syria, where they have been fighting the Islamic State.\n",
      "\n",
      "He called Mr. Mattis “an island of stability amidst the chaos of the Trump administration.”\n",
      "\n",
      "“As we’ve seen with the President’s haphazard approach to Syria, our national defense is too important to be subjected to the President’s erratic whims,” Mr. Warner wrote in the Twitter post.\n",
      "\n",
      "Mr. Mattis had told close friends that he would continue in the job despite his deteriorating relationship with Mr. Trump, because he viewed his commitment to protecting the Defense Department and its 1.3 million active duty service members as paramount.\n",
      "\n",
      "Senator Marco Rubio, Republican of Florida and member of the Senate Foreign Relations Committee, said on Twitter that Mr. Mattis’s resignation letter “makes it abundantly clear that we are headed towards a series of grave policy errors which will endanger our nation, damage our alliances & empower our adversaries.”\n",
      "\n",
      "As defense secretary, Mr. Mattis oversaw the world’s most powerful military, supervising active-duty troops based in the United States and deployed worldwide, including in war zones in Afghanistan, Iraq and on the Saudi border with Yemen.\n",
      "\n",
      "Over the past six months, the president and the defense chief have also found themselves at odds over NATO policy, whether to resume large-scale military exercises with South Korea and, privately, whether Mr. Trump’s decision to withdraw the United States from the Iran nuclear deal has proved effective.\n",
      "\n",
      "The president’s tweets announcing the departure of his defense secretary shocked officials at the Pentagon, who as recently as Thursday afternoon were insisting that Mr. Mattis had no intention of resigning his post, despite his anger at Mr. Trump’s decision, announced on Wednesday, to withdraw American troops from Syria.\n",
      "\n",
      "Mr. Trump announced Mr. Mattis’s resignation in two tweets Thursday evening, and said the retired four-star Marine general turned Pentagon chief will leave at the end of February.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#takes 33% of the most relevant content from the original article\n",
    "for i in range(int(len(sentence_vectors)*(1/3))):\n",
    "    print(ranked_sentences[i][1], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
