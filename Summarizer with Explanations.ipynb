{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform extractive text summarization using the TextRank algorithm which uses a graph-based ranking model which requires no training, using GloVe v1.2 pre-trained word vectors, and performing various natural language preprocessing & tokenization using NLTK library.\n",
    "\n",
    "Graph based ranking algorithms allow knowledge about the text as a whole and the relationship between different parts of a text to be used in making specific local ranking decisions.  It does so by taking into account global information recursively computed from the entire graph to evaluate the importance of a vertex within a graph, rather than relying only on local information.\n",
    "\n",
    "Traditional word vector techniques depend on the distance or angle between pairs of word vectors to determine the strength of a set of word representations.  Glove attempts to uncover more of the language structure by examining not only the scalar difference but various dimensions of difference.  It does this by examining the ratio of the co-occurrence probability between pairings rather than just the probabilities alone.  A weighted least squares regression is then applied to remove the noise.  Dimensionality reduction is applied to the co-occurrence matrix to yield a lower dimensional matrix such that each vector represents a word.\n",
    "\n",
    "Commonly, the ROUGE Metric is used to evaluate summarizer performance, but we did not have immediate resources to test it.  Subjectively, the performance seems reasonably good.  When summarizing NY Times articles, they were significantly shorter (33% of original article length was chosen), were easy to read, and conveyed the most important ideas within the text.\n",
    "\n",
    "This summarizer does not understand or recognize the grammatical structure of language so it cannot extract this type of semantic information and its importance in relating sentences; eg: pronoun references.  Abstractive text summarization, possibly with GANs would be an interesting future project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transfer Learning\\nGo to the profile of Niklas Donges\\nNiklas Donges\\nApr 23, 2018\\nTransfer Learning is the reuse of a pre-trained model on a new problem.',\n",
       " 'It is currently very popular in the field of Deep Learning because it enables you to train Deep Neural Networks with comparatively little data.',\n",
       " 'This is very useful since most real-world problems typically do not have millions of labeled data points to train such complex models.',\n",
       " 'This blog post is intended to give you an overview of what Transfer Learning is, how it works, why you should use it and when you can use it.',\n",
       " 'It will introduce you to the different approaches of Transfer Learning and provide you with some resources on already pre-trained models.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read text from file as a string\n",
    "with open(\"../Text-Summarizer/articles/article.txt\", \"r\") as myfile:\n",
    "    article=myfile.read()\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = sent_tokenize(article)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten list of sentences\n",
    "def flatten_list(sentences):\n",
    "    new_sentences = []\n",
    "    for s in sentences:\n",
    "        new_sentences.append(sent_tokenize(s))\n",
    "\n",
    "    new_sentences = [y for x in new_sentences for y in x] # flatten list\n",
    "    return new_sentences\n",
    "\n",
    "new_sentences = flatten_list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, numbers, and special characters\n",
    "clean_sentences = pd.Series(new_sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# Make lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jhsoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords, common words that we don't want to affect the ranking.  We add them back later\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence_vectors)):\n",
    "    sentence_vectors[i] = sentence_vectors[i].reshape(1, 100)\n",
    "\n",
    "print(len(sentence_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "print(sim_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity computes the similarity between vectors based on the degree of orthogonality between vectors where a cosine of 1 is identical and a cosine of 0 is orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i], sentence_vectors[j])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix to TextRank\n",
    "import networkx as nx\n",
    "\n",
    "# Generate graph from matrix representation\n",
    "nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "# Apply TextRank/PageRank algorithm to graph\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output below: 38% of most relevant content from original article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is very useful since most real-world problems typically do not have millions of labeled data points to train such complex models.\n",
      "\n",
      "This blog post is intended to give you an overview of what Transfer Learning is, how it works, why you should use it and when you can use it.\n",
      "\n",
      "It will introduce you to the different approaches of Transfer Learning and provide you with some resources on already pre-trained models.\n",
      "\n",
      "In Transfer Learning, the knowledge of an already trained Machine Learning model is applied to a different but related problem.\n",
      "\n",
      "For example, if you trained a simple classifier to predict whether an image contains a backpack, you could use the knowledge that the model gained during its training to recognize other objects like sunglasses.\n",
      "\n",
      "The general idea is to use knowledge, that a model has learned from a task where a lot of labeled training data is available, in a new task where we don’t have a lot of data.\n",
      "\n",
      "Transfer Learning is mostly used in Computer Vision and Natural Language Processing Tasks like Sentiment Analysis, because of the huge amount of computational power that is needed for them.\n",
      "\n",
      "Transfer Learning can be seen as a ‘design methodology’ within Machine Learning like for example, active learning.\n",
      "\n",
      "In Transfer Learning, we try to transfer as much knowledge as possible from the previous task, the model was trained on, to the new task at hand.\n",
      "\n",
      "The main advantages are basically that you save training time, that your Neural Network performs better in most cases and that you don’t need a lot of data.\n",
      "\n",
      "Usually, you need a lot of data to train a Neural Network from scratch but you don’t always have access to enough data.\n",
      "\n",
      "That is where Transfer Learning comes into play because with it you can build a solid machine Learning model with comparatively little training data because the model is already pre-trained.\n",
      "\n",
      "Therefore you also save a lot of training time, because it can sometimes take days or even weeks to train a deep Neural Network from scratch on a complex task.\n",
      "\n",
      "According to Demis Hassabis, the CEO of DeepMind Technologies, Transfer is also one of the most promising techniques that could someday lead us to Artificial General Intelligence (AGI):\n",
      "\n",
      "\n",
      "When you should use it\n",
      "As it is always the case in Machine Learning, it is hard to form rules that are generally applicable.\n",
      "\n",
      "You would typically use Transfer Learning when (a) you don’t have enough labeled training data to train your network from scratch and/or (b) there already exists a network that is pre-trained on a similar task, which is usually trained on massive amounts of data.\n",
      "\n",
      "Note that Transfer Learning only works if the features learned from the first task are general, meaning that they can be useful for another related task as well.\n",
      "\n",
      "Training a Model to Reuse it\n",
      "Imagine you want to solve Task A but don’t have enough data to train a Deep Neural Network.\n",
      "\n",
      "One way around this issue would be to find a related Task B, where you have an abundance of data.\n",
      "\n",
      "Then you could train a Deep Neural Network on Task B and use this model as starting point to solve your initial Task A.\n",
      "\n",
      "Alternatively, you could also just change and re-train different task-specific layers and the output layer.\n",
      "\n",
      "How many layers you reuse and how many you are training again, depends like I already said on your problem and it is therefore hard to form a general rule.\n",
      "\n",
      "Keras, for example, provides nine pre-trained models that you can use for Transfer Learning, Prediction, feature extraction and fine-tuning.\n",
      "\n",
      "This type of Transfer Learning is most commonly used throughout Deep Learning.\n",
      "\n",
      "Feature Extraction\n",
      "Another approach is to use Deep Learning to discover the best representation of your problem, which means finding the most important features.\n",
      "\n",
      "This approach is also known as Representation Learning and can often result in a much better performance than can be obtained with hand-designed representation.\n",
      "\n",
      "Note that this does not mean that Feature Engineering and Domain knowledge isn’t important anymore because you still have to decide which features you put into your Network.\n",
      "\n",
      "A representation learning algorithm can discover a good combination of features within a very short timeframe, even for complex tasks which would otherwise require a lot of human effort.\n",
      "\n",
      "The learned representation can then be used for other problems as well.\n",
      "\n",
      "You simply use the first layers to spot the right representation of features but you don’t use the output of the network because it is too task-specific.\n",
      "\n",
      "This approach is mostly used in Computer Vision because it can reduce the size of your dataset, which decreases computation time and makes it more suitable for traditional algorithms as well.\n",
      "\n",
      "We talked about why it can reduce the size of your dataset, why it decreases training time and why you also need less data when you use it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#takes 38% of the most relevant content from the original article\n",
    "#update retains original article sentence order\n",
    "\n",
    "#score and sort each sentence by score along with original sentence order\n",
    "ranked_sentences = sorted(((i,scores[i],s) for i,s in enumerate(sentences)), reverse=True, key=lambda x: x[1])\n",
    "\n",
    "#keep only the top X% of ranked sentences\n",
    "ranked_sentences = ranked_sentences[:int(len(sentence_vectors)*.38)]\n",
    "\n",
    "#resort the ranked sentences in the original order\n",
    "ranked_sentences = sorted(ranked_sentences, key=lambda x: x[0])\n",
    "\n",
    "#print all the ranked sentences which meet the score threshold\n",
    "for i in range(len(ranked_sentences)):\n",
    "    print(ranked_sentences[i][2], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
