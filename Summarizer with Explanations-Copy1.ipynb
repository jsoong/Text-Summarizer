{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform extractive text summarization using the TextRank algorithm which uses a graph-based ranking model which requires no training, using GloVe v1.2 pre-trained word vectors, and performing various natural language preprocessing & tokenization using NLTK library.\n",
    "\n",
    "Graph based ranking algorithms allow knowledge about the text as a whole and the relationship between different parts of a text to be used in making specific local ranking decisions.  It does so by taking into account global information recursively computed from the entire graph in order to evaluate the importance of a vertex within a graph, rather than relying only on local information.\n",
    "\n",
    "Traditional word vector techniques depend on the distance or angle between pairs of word vectors to determine strength of a set  word representations.  Glove attempts to uncover more of the language structure by examining not only the scalar difference but various dimensions of difference.  It does this by examining the ratio of the co-occurance probability between pairings rather the just the probabilties themselves.  A weighted least squares regression is then applied to remove the noise.  Dimesionality reduction is applied to the co-occurance matrix to yield a lower dimensional matrix such that each vector represents a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "#article = open(\"../Text-Summarizer/articles/research paper.txt\", \"r\")\n",
    "\n",
    "#sentences = []\n",
    "\n",
    "with open(\"../Text-Summarizer/articles/wired article.txt\", \"r\", encoding=\"utf-8\") as myfile:\n",
    "    article=myfile.read()\n",
    "\n",
    "sentences = sent_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(sentences):\n",
    "    new_sentences = []\n",
    "    for s in sentences:\n",
    "        new_sentences.append(sent_tokenize(s))\n",
    "\n",
    "    new_sentences = [y for x in new_sentences for y in x] # flatten list\n",
    "    return new_sentences\n",
    "\n",
    "new_sentences = flatten_list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(new_sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence_vectors)):\n",
    "    sentence_vectors[i] = sentence_vectors[i].reshape(1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity computes the similarity between vectors based on the degree of orthogonality between vectors where a cosine of 1 is identical and a cosine of 0 is orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sim_matrix(n, sentence_vectors):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sim_mat = np.zeros([n, n])\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i], sentence_vectors[j])[0,0]\n",
    "    return sim_mat\n",
    "\n",
    "sim_mat = create_sim_matrix(len(sentences), sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS STARTUP IS CHALLENGING GOOGLE MAPS—AND IT NEEDS YOU\n",
      "\n",
      "StreetCred's MapNYC program is an effort to find out what might motivate map enthusiasts, crypto-lovers, maybe even people who hadn’t the faintest about either, to feed it data.MUIRIS WOULFE/GETTY IMAGES\n",
      "A WHOLE LIFETIME in New York City, and Christiana Ting didn’t realize just how many urgent care facilities there were until the app told her to start looking for them.\n",
      "\n",
      "And it’s still reviewing lessons learned in New York: The company says the MapNYC project generated over 20,000 places in four weeks, some validated by three users.\n",
      "\n",
      "The company says it will contribute that info back to OpenStreetMaps under a licensing agreement designed to make it easier for people and companies to share and collaborate when working with data.\n",
      "\n",
      "It requires a lot of eyes on the ground and people actually observing changes in the field.”\n",
      "\n",
      "Which is why Meech says he came up with MapNYC: an effort to find out what might motivate map enthusiasts, crypto-lovers, maybe even people who hadn’t the faintest about either, to feed it data.\n",
      "\n",
      "Maybe a client found a data set online or purchased one from another company.\n",
      "\n",
      "“I took mapping for granted until I started the competition,” Ting says, even though she pulls up Google Maps at least twice a day.\n",
      "\n",
      "“There’s a lot of companies, none of whom I can name, who have location data, and that data needs improvement,” says Randy Meech, CEO of the small startup.\n",
      "\n",
      "“I’m not sure what that says about the neighborhood where I work.”\n",
      "\n",
      "Ting was one of 761 New Yorkers who downloaded, played with, and occasionally became obsessed with an app called MapNYC this fall, vying for their share of an 8-bitcoin prize (worth about $50,000 at the time).\n",
      "\n",
      "“But it’s such an inconvenience if the info on the map is wrong, especially in a place like New York, that’s changing all the time.”\n",
      "\n",
      "For regular folk, detailed, reliable mapping info is helpful.\n",
      "\n",
      "StreetCred’s main research question: How do you convince regular people to build and verify mapping data?\n",
      "\n",
      "“They were giving extra points for medical offices, and I found them, I think, on every block,” she says.\n",
      "\n",
      "StreetCred wants to make that info free for customers who don’t need that much data and cheaper for those that do.\n",
      "\n",
      "So StreetCred might offer a future mapping army an extra crypto incentive to find, validate, and label those locations.\n",
      "\n",
      "The month-long contest, run by a new mapping startup called StreetCred, was really an experiment.\n",
      "\n",
      "Others use products that rely on base maps—think Uber, the Weather Channel, your car’s navigation system—and require up-to-date location data.\n",
      "\n",
      "If a company making a VR game for kids needs to know the location of every playground in Cincinnati, there’s no guarantee the volunteers will plug that in.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#takes 33% of the most relevant content from the original article\n",
    "for i in range(int(len(sentence_vectors)*(1/3))):\n",
    "    print(ranked_sentences[i][1], end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
